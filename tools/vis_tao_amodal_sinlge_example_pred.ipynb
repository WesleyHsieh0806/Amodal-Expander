{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengyeh/.conda/envs/TAO-Amodal-gtr/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from detectron2.data import detection_utils as utils\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import json\n",
    "from collections import defaultdict\n",
    "annotation_file = '/home/chengyeh/TAO-Amodal-Root/TAO-GTR/datasets/tao/amodal_annotations/validation_with_freeform_amodal_boxes_Aug10_2022_GTR_lvis_v1.json'\n",
    "prediction_file = '/data3/chengyeh/TAO-Amodal-experiments/GTR/AmodalExpander/TAO-Amodal/ModalMatch/PasteNOcclude/GTR_TAO_Amodal_Expander_PasteNOcclude/iter45000/inference_tao_amodal_val_v1/lvis_instances_results.json'\n",
    "\n",
    "with open(annotation_file, 'r') as f:\n",
    "    tao_amodal = json.load(f)\n",
    "\n",
    "with open(prediction_file, 'r') as f:\n",
    "    prediction = json.load(f)\n",
    "\n",
    "modal_prediction_file = '/data3/chengyeh/TAO-Amodal-experiments/GTR/GTR_TAO_Amodal/inference_tao_amodal_val_v1/lvis_instances_results.json'\n",
    "with open(modal_prediction_file, 'r') as f:\n",
    "    modal_prediction = json.load(f)\n",
    "\n",
    "img_id_to_img = {image['id']: image for image in tao_amodal['images']}\n",
    "vname_to_img_ids = defaultdict(list)\n",
    "img_name_to_id = defaultdict(int)\n",
    "for img in tao_amodal['images']:\n",
    "    vname_to_img_ids[img['video']].append(img['id'])\n",
    "    img_name_to_id[img['file_name']] = img['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_id_to_prediction = defaultdict(list)\n",
    "img_id_to_modal_prediction = defaultdict(list)\n",
    "img_id_to_ann = defaultdict(list)\n",
    "\n",
    " \n",
    "for pred in prediction:\n",
    "    img_id_to_prediction[pred['image_id']].append(pred)\n",
    "\n",
    "for pred in modal_prediction:\n",
    "    img_id_to_modal_prediction[pred['image_id']].append(pred)\n",
    "\n",
    "for ann in tao_amodal['annotations']:\n",
    "    img_id_to_ann[ann['image_id']].append(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "def colormap(rgb=False, as_int=False):\n",
    "    color_list = np.array(\n",
    "        [\n",
    "            0.000, 0.447, 0.741,\n",
    "            0.850, 0.325, 0.098,\n",
    "            0.929, 0.694, 0.125,\n",
    "            0.494, 0.184, 0.556,\n",
    "            0.466, 0.674, 0.188,\n",
    "            0.301, 0.745, 0.933,\n",
    "            0.635, 0.078, 0.184,\n",
    "            0.300, 0.300, 0.300,\n",
    "            0.600, 0.600, 0.600,\n",
    "            1.000, 0.000, 0.000,\n",
    "            1.000, 0.500, 0.000,\n",
    "            0.749, 0.749, 0.000,\n",
    "            0.000, 1.000, 0.000,\n",
    "            0.000, 0.000, 1.000,\n",
    "            0.667, 0.000, 1.000,\n",
    "            0.333, 0.333, 0.000,\n",
    "            0.333, 0.667, 0.000,\n",
    "            0.333, 1.000, 0.000,\n",
    "            0.667, 0.333, 0.000,\n",
    "            0.667, 0.667, 0.000,\n",
    "            0.667, 1.000, 0.000,\n",
    "            1.000, 0.333, 0.000,\n",
    "            1.000, 0.667, 0.000,\n",
    "            1.000, 1.000, 0.000,\n",
    "            0.000, 0.333, 0.500,\n",
    "            0.000, 0.667, 0.500,\n",
    "            0.000, 1.000, 0.500,\n",
    "            0.333, 0.000, 0.500,\n",
    "            0.333, 0.333, 0.500,\n",
    "            0.333, 0.667, 0.500,\n",
    "            0.333, 1.000, 0.500,\n",
    "            0.667, 0.000, 0.500,\n",
    "            0.667, 0.333, 0.500,\n",
    "            0.667, 0.667, 0.500,\n",
    "            0.667, 1.000, 0.500,\n",
    "            1.000, 0.000, 0.500,\n",
    "            1.000, 0.333, 0.500,\n",
    "            1.000, 0.667, 0.500,\n",
    "            1.000, 1.000, 0.500,\n",
    "            0.000, 0.333, 1.000,\n",
    "            0.000, 0.667, 1.000,\n",
    "            0.000, 1.000, 1.000,\n",
    "            0.333, 0.000, 1.000,\n",
    "            0.333, 0.333, 1.000,\n",
    "            0.333, 0.667, 1.000,\n",
    "            0.333, 1.000, 1.000,\n",
    "            0.667, 0.000, 1.000,\n",
    "            0.667, 0.333, 1.000,\n",
    "            0.667, 0.667, 1.000,\n",
    "            0.667, 1.000, 1.000,\n",
    "            1.000, 0.000, 1.000,\n",
    "            1.000, 0.333, 1.000,\n",
    "            1.000, 0.667, 1.000,\n",
    "            0.167, 0.000, 0.000,\n",
    "            0.333, 0.000, 0.000,\n",
    "            0.500, 0.000, 0.000,\n",
    "            0.667, 0.000, 0.000,\n",
    "            0.833, 0.000, 0.000,\n",
    "            1.000, 0.000, 0.000,\n",
    "            0.000, 0.167, 0.000,\n",
    "            0.000, 0.333, 0.000,\n",
    "            0.000, 0.500, 0.000,\n",
    "            0.000, 0.667, 0.000,\n",
    "            0.000, 0.833, 0.000,\n",
    "            0.000, 1.000, 0.000,\n",
    "            0.000, 0.000, 0.167,\n",
    "            0.000, 0.000, 0.333,\n",
    "            0.000, 0.000, 0.500,\n",
    "            0.000, 0.000, 0.667,\n",
    "            0.000, 0.000, 0.833,\n",
    "            0.000, 0.000, 1.000,\n",
    "            0.000, 0.000, 0.000,\n",
    "            0.143, 0.143, 0.143,\n",
    "            0.286, 0.286, 0.286,\n",
    "            0.429, 0.429, 0.429,\n",
    "            0.571, 0.571, 0.571,\n",
    "            0.714, 0.714, 0.714,\n",
    "            0.857, 0.857, 0.857,\n",
    "            1.000, 1.000, 1.000\n",
    "        ]\n",
    "    ).astype(np.float32)\n",
    "    color_list = color_list.reshape((-1, 3)) * 255\n",
    "    if not rgb:\n",
    "        color_list = color_list[:, ::-1]\n",
    "    if as_int:\n",
    "        color_list = color_list.astype(np.uint8)\n",
    "    return color_list\n",
    "color_generator = itertools.cycle(colormap(rgb=True).tolist())\n",
    "color_map = collections.defaultdict(lambda: next(color_generator))\n",
    "_BLACK = (0, 0, 0)\n",
    "_RED = (255, 0, 0)\n",
    "_BLUE = (0, 0, 255)\n",
    "_GRAY = (218, 227, 218)\n",
    "_GREEN = (18, 127, 15)\n",
    "_WHITE = (255, 255, 255)\n",
    "\n",
    "_COLOR1 = tuple(255*x for x in (0.000, 0.447, 0.741))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98596, 98597, 98598, 98599, 98600, 98601, 98602, 98603, 98604, 98605, 98606, 98607, 98608, 98609, 98610, 98611, 98612, 98613, 98614, 98615, 98616, 98617, 98618, 98619, 98620, 98621, 98622, 98623, 98624, 98625, 98626, 98627, 98628]\n",
      "50646\n",
      "[{'bbox': [1480.2868090183247, 651.9724419269336, 71.81004189447299, 218.49944464041312], 'area': 15690, 'iscrowd': 0, 'id': '1200_46777_7996', 'image_id': 46777, 'category_id': 793, 'track_id': 7996, '_scale_uuid': 'ceffaa18-3415-4e5b-ba67-4bcd689be7de', 'scale_category': 'person', 'video_id': 1200, 'amodal_bbox': [1480.2868090183247, 651.9724419269336, 71.81004189447299, 218.49944464041312], 'amodal_is_uncertain': False}, {'bbox': [323.0, 639.0, 96.28102491454553, 250.22878239394902], 'area': 24092, 'iscrowd': 0, 'id': '1200_46777_7997', 'image_id': 46777, 'category_id': 793, 'track_id': 7997, '_scale_uuid': '740a7186-e6da-4653-b8d1-3adfc65e7b95', 'scale_category': 'person', 'video_id': 1200, 'amodal_bbox': [323.0, 639.0, 96.28102491454553, 250.22878239394902], 'amodal_is_uncertain': False}, {'bbox': [789.0, 722.4218875809149, 65.23262958210216, 138.57811241908507], 'area': 9039, 'iscrowd': 0, 'id': '1200_46777_7999', 'image_id': 46777, 'category_id': 793, 'track_id': 7999, '_scale_uuid': 'bed4e81d-382d-4dfb-b2ad-66a9b875b2a6', 'scale_category': 'person', 'video_id': 1200, 'amodal_bbox': [789.0, 722.4218875809149, 65.23262958210216, 138.57811241908507], 'amodal_is_uncertain': False}, {'image_id': 46777, 'category_id': 61, 'track_id': 7998, 'scale_category': 'object_moved_by_person', 'video_id': 1200, 'id': '1200_46777_7998', 'amodal_bbox': [550.3707245889938, 778.503506414429, 60.46990152644503, 49.131794990236585], 'amodal_is_uncertain': False, 'bbox': [550.3707245889938, 778.503506414429, 60.46990152644503, 49.131794990236585], 'area': 2970}]\n",
      "{'id': 98610, 'video': 'val/Charades/B0MFE', '_scale_task_id': '5d92c19fae6dae1ed4d794ff', 'width': 1280, 'height': 720, 'file_name': 'val/Charades/B0MFE/frame0421.jpg', 'frame_index': 420, 'license': 0, 'video_id': 2648, 'neg_category_ids': [679, 735, 278], 'not_exhaustive_category_ids': [1108, 110]}\n"
     ]
    }
   ],
   "source": [
    "def transparent_except_bbox(image, all_annos, opacity=0.6):\n",
    "    with_fill = np.ones_like(image) * 255\n",
    "    for ann in all_annos:\n",
    "        oy, ox = image.shape[:2]\n",
    "        oy, ox = int(oy / 4), int(ox / 4)\n",
    "\n",
    "        box = ann['bbox']\n",
    "        box = [max(0, box[0]+ox), max(0, box[1]+oy), min(image.shape[1], box[2]+box[0]+ox), min(image.shape[0], box[3]+box[1]+oy)]\n",
    "        with_fill[int(box[1]):int(box[3]), int(box[0]):int(box[2])] = image[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "    \n",
    "    image = cv2.addWeighted(with_fill, opacity, image,\n",
    "                                1 - opacity, 0, image)\n",
    "    return image\n",
    "\n",
    "def vis_image_and_annotations(image, all_annos, show_track=False, show_image_id=True, transparent=False):\n",
    "    height, width = image.shape[:2]\n",
    "    new_image = np.ones([height * 2, width * 2, 3], dtype=np.uint8) * 255\n",
    "    startx = int(width / 2)\n",
    "    endx = startx + width\n",
    "    starty = int(height / 2) \n",
    "    endy = starty + height\n",
    "    new_image[starty: endy, startx: endx, :] = image\n",
    "\n",
    "    with open('/home/chengyeh/TAO-Amodal-Root/TAO-GTR/datasets/lvis/lvis_v1_train+coco_box.json', 'r') as f:\n",
    "        lvis = json.load(f)\n",
    "        id_to_cat_name = {cat['id']: cat['name'] for cat in lvis['categories']}\n",
    "\n",
    "    if transparent:\n",
    "        new_image = transparent_except_bbox(new_image, all_annos, opacity=0.45)\n",
    "\n",
    "    for ann in all_annos:\n",
    "        # if ann['category_id'] != 793 or ann['track_id'] in [1000001, 1000004, 1000007, 1000008]:\n",
    "        #     continue\n",
    "        oy, ox = new_image.shape[:2]\n",
    "        oy, ox = int(oy / 4), int(ox / 4)\n",
    "\n",
    "        box = ann['bbox']\n",
    "        box = [box[0]+ox, box[1]+oy, box[2]+box[0]+ox, box[3]+box[1]+oy]\n",
    "        new_image = vis_bbox(new_image, box, fill_opacity=0.32, fill_color=None, border_color=color_map[ann['track_id']], thickness=1)\n",
    "        # new_image = vis_bbox(new_image, box, fill_opacity=0.32, fill_color=None, border_color=(255, 102, 255), thickness=20)\n",
    "        \n",
    "        if not show_track:\n",
    "            # pass\n",
    "            new_image = vis_class(new_image, box[:2], id_to_cat_name[ann['category_id']])\n",
    "        else:\n",
    "            new_image = vis_class(new_image, box[:2], str(ann['track_id']))\n",
    "    \n",
    "\n",
    "    # Check bounding box, Check Category\n",
    "    if all_annos and show_image_id:\n",
    "        new_image = vis_class(new_image, [int(startx + (endx - startx)* 0.4 ), starty // 2], str(all_annos[-1]['image_id']),\n",
    "                                    bg_color=(255, 255, 255),\n",
    "                                    text_color=(0, 0, 0),\n",
    "                                    font_scale=2.5,\n",
    "                                    thickness=3)\n",
    "    pil_image = Image.fromarray(new_image)\n",
    "    return pil_image\n",
    "\n",
    "def vis_class(image,\n",
    "              pos,\n",
    "              class_str,\n",
    "              font_scale=2.0,\n",
    "              bg_color=_BLACK,\n",
    "              text_color=_GRAY,\n",
    "              thickness=4):\n",
    "    \"\"\"Visualizes the class.\"\"\"\n",
    "    x, y = int(pos[0]), int(pos[1])\n",
    "    # Compute text size.\n",
    "    txt = class_str\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    ((txt_w, txt_h), _) = cv2.getTextSize(txt, font, font_scale, 1)\n",
    "    # Place text background.\n",
    "    back_tl = x, y - int(1.5 * txt_h)\n",
    "    back_br = x + txt_w, y - int(0.3 * txt_h)\n",
    "    # Show text.\n",
    "    txt_tl = x, y - int(0.6 * txt_h)\n",
    "    cv2.rectangle(image, back_tl, back_br, bg_color, -1)\n",
    "    cv2.putText(image,\n",
    "                txt,\n",
    "                txt_tl,\n",
    "                font,\n",
    "                font_scale,\n",
    "                text_color,\n",
    "                thickness=thickness,\n",
    "                lineType=cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "def vis_bbox(image,\n",
    "             box,\n",
    "             border_color=_BLACK,\n",
    "             fill_color=_COLOR1,\n",
    "             fill_opacity=0.65,\n",
    "             thickness=2):\n",
    "    \"\"\"Visualizes a bounding box.\"\"\"\n",
    "    x0, y0, x1, y1 = box\n",
    "    x1, y1 = int(x1), int(y1)\n",
    "    x0, y0 = int(x0), int(y0)\n",
    "    # Draw border\n",
    "    if fill_opacity > 0 and fill_color is not None:\n",
    "        with_fill = image.copy()\n",
    "        with_fill = cv2.rectangle(with_fill, (x0, y0), (x1, y1),\n",
    "                                  tuple(fill_color), cv2.FILLED)\n",
    "        image = cv2.addWeighted(with_fill, fill_opacity, image,\n",
    "                                1 - fill_opacity, 0, image)\n",
    "        \n",
    "    image = cv2.rectangle(image, (x0, y0), (x1, y1), tuple(border_color),\n",
    "                          thickness)\n",
    "    return image\n",
    "\n",
    "save_dir = os.path.dirname(os.path.dirname(prediction_file))\n",
    "display_on_notebook = True\n",
    "IMAGE_ROOT='/compute/trinity-1-38/chengyeh/TAO/frames'\n",
    "print(vname_to_img_ids['val/Charades/B0MFE'])\n",
    "print(img_name_to_id['val/AVA/keUOiCcHtoQ_scene_23_102872-103750/frame0301.jpg'])\n",
    "print(img_id_to_ann[46777])\n",
    "print(img_id_to_img[98610])\n",
    "\n",
    "# # Randomly visualize 100 images\n",
    "# img_ids = np.random.choice(list(img_id_to_img.keys()), size=100)\n",
    "# img_ids = list(sorted(vname_to_img_ids['val/AVA/keUOiCcHtoQ_scene_23_102872-103750']))\n",
    "# img_ids = [46777]\n",
    "\n",
    "\n",
    "\n",
    "# filter_with_track = True\n",
    "# track_id = [120002060002]\n",
    "# show_track = False\n",
    "# show_image_id = False\n",
    "# score_thresh = 0.5\n",
    "# transparent = True\n",
    "\n",
    "# with open(os.path.join(save_dir, 'selected_prediction.html'), 'w') as f:\n",
    "#     f.write('<!DOCTYPE html>\\n')\n",
    "#     f.write('<html>\\n')\n",
    "#     f.write('<body>\\n')\n",
    "#     f.write('<style>\\n')\n",
    "#     f.write('table, th, td {\\n')\n",
    "#     f.write('border:1px solid black;\\n')\n",
    "#     f.write('}\\n')\n",
    "#     f.write('</style>\\n')\n",
    "#     f.write('<body>\\n')\n",
    "#     f.write('<h2>Qualitative Results</h2>\\n\\n')\n",
    "#     f.write(('<table style=\"width:100%\">\\n'\n",
    "#                 '<tr>\\n'\n",
    "#                 '<th>Modal</th>\\n'\n",
    "#                 '<th>Amodal</th>\\n'\n",
    "#                 '<th>GT</th>\\n'\n",
    "#                 '</tr>\\n')\n",
    "#                 )\n",
    "#     for i,img_id in enumerate(img_ids):\n",
    "#         print(\"{}/{}\".format(i + 1, len(img_ids)), end='\\r')\n",
    "#         selected_prediction = [pred for pred in img_id_to_prediction[img_id] if pred['score'] >= score_thresh]\n",
    "#         if filter_with_track:\n",
    "#             selected_prediction = [pred for pred in img_id_to_prediction[img_id] if pred['track_id'] in track_id]    \n",
    "#         for pred in selected_prediction:\n",
    "#             print(pred)\n",
    "\n",
    "#         img_path = os.path.join(IMAGE_ROOT, img_id_to_img[img_id]['file_name'])\n",
    "#         np_image = utils.read_image(img_path)\n",
    "#         pil_image = vis_image_and_annotations(np_image, selected_prediction, show_track, show_image_id, transparent)\n",
    "#         amodal_img_path = os.path.join(save_dir, 'predictions', img_id_to_img[img_id]['file_name'].replace('.', '_amodal.'))\n",
    "#         if not os.path.isdir(os.path.dirname(amodal_img_path)):\n",
    "#             os.makedirs(os.path.dirname(amodal_img_path))\n",
    "#         pil_image.save(amodal_img_path)\n",
    "\n",
    "#         # Amodal Predictions\n",
    "#         if display_on_notebook:\n",
    "#             display(pil_image)\n",
    "\n",
    "#         # Modal Predictions\n",
    "#         new_modal_prediction = [pred for pred in img_id_to_modal_prediction[img_id] if pred['score'] >= score_thresh]\n",
    "#         if filter_with_track:\n",
    "#             new_modal_prediction = [pred for pred in img_id_to_modal_prediction[img_id] if pred['track_id'] in track_id]\n",
    "#         for pred in new_modal_prediction:\n",
    "#             if pred['track_id'] == 157007920130:\n",
    "#                 pred['track_id'] -= 1\n",
    "#             print(pred)\n",
    "#         pil_image = vis_image_and_annotations(np_image, new_modal_prediction, show_track, show_image_id, transparent)\n",
    "#         pil_image.save(os.path.join(save_dir, 'predictions', img_id_to_img[img_id]['file_name'].replace('.', '_modal.')))\n",
    "\n",
    "#         if display_on_notebook:\n",
    "#             display(pil_image)\n",
    "\n",
    "#         # GT \n",
    "#         selected_anns = [pred for pred in img_id_to_ann[img_id]]\n",
    "#         # for ann in selected_anns:\n",
    "#         #     print(ann)\n",
    "#         pil_image = vis_image_and_annotations(np_image, selected_anns, show_track)\n",
    "#         pil_image.save(os.path.join(save_dir, 'predictions', img_id_to_img[img_id]['file_name'].replace('.', '_gt.')))\n",
    "#         if display_on_notebook:\n",
    "#             display(pil_image)\n",
    "        \n",
    "#         f.write('<tr>\\n')\n",
    "#         f.write('<td><img src=\\\"{}\\\" alt=\"Modal\" style=\"width:1200px;\"></td>\\n'.format(os.path.join('predictions', img_id_to_img[img_id]['file_name'].replace('.', '_modal.'))))\n",
    "#         f.write('<td><img src=\\\"{}\\\" alt=\"Amodal\" style=\"width:1200px;\"></td>\\n'.format(os.path.join('predictions', img_id_to_img[img_id]['file_name'].replace('.', '_amodal.'))))\n",
    "#         f.write('<td><img src=\\\"{}\\\" alt=\"GT\" style=\"width:1200px;\"></td>\\n'.format(os.path.join('predictions', img_id_to_img[img_id]['file_name'].replace('.', '_gt.'))))\n",
    "#         f.write('/<tr>\\n')\n",
    "    \n",
    "#     f.write(('</table>\\n\\n'\n",
    "#             '</body>\\n'\n",
    "#             '</html>'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAO-Amodal-gtr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
